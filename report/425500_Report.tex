\documentclass[12pt,oneside]{book} % for one-sided printing

\usepackage{blindtext}% Just used so we can generate some example text
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage[export]{adjustbox}
\usepackage{lipsum}
\usepackage{booktabs}  % For better quality tables
\usepackage{tabularx}  % for the X column type
\usepackage{listings}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{xfrac}
\usepackage{indentfirst}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{a4paper, margin=1in}

% Place style file after other packages.
\usepackage{cranfieldthesis}
\usepackage{lscape} % for landscape pages
\usepackage{float}
\usepackage[toc,title,page]{appendix}

% Couleurs personnalisées
\definecolor{backcolour}{rgb}{0.96, 0.96, 0.96} % Fond très clair
\definecolor{codegray}{rgb}{0.47, 0.47, 0.47}   % Commentaires et numéros de ligne
\definecolor{codegreen}{rgb}{0.25, 0.50, 0.35}  % Commentaires
\definecolor{codeblue}{rgb}{0.26, 0.44, 0.58}   % Mots-clés
\definecolor{codepurple}{rgb}{0.50, 0, 0.50}    % Identificateurs
\definecolor{codeteal}{rgb}{0, 0.5, 0.5}        % Chaînes de caractères
\definecolor{terminalback}{rgb}{0.05, 0.05, 0.05} % Fond très sombre pour le terminal
\definecolor{terminaltext}{rgb}{0.7, 0.7, 0.7}    % Texte clair pour le terminal
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\definecolor{terminalbgcolor}{HTML}{330033}
\definecolor{terminalrulecolor}{HTML}{000099}

\lstdefinestyle{bashstyle}{
    language=bash,
    backgroundcolor=\color{backcolour},
    basicstyle=\ttfamily\scriptsize,
    keywordstyle=\color{blue},
    stringstyle=\color{red},
    identifierstyle=\color{codepurple},
    commentstyle=\color{codegreen},
    morecomment=[l]{\#},   % Define comment style
    frame=single,          % adds a frame around the code
    rulecolor=\color{gray},% if not set, the frame-color may be changed on line-breaks
    breakatwhitespace=false,
    breaklines=true,       % sets automatic line breaking
    captionpos=b,          % sets the caption-position to bottom
    keepspaces=true,       % keeps spaces in text
    showspaces=false,      % show spaces everywhere adding particular underscores
    showstringspaces=false % underline spaces within strings only
}

\lstdefinestyle{cstyle}{
    language=C++,
    basicstyle=\ttfamily\scriptsize,
    keywordstyle=\color{blue},
    backgroundcolor=\color{backcolour},
    stringstyle=\color{red},
    commentstyle=\color{codegreen},
    morecomment=[l][\color{magenta}]{\#},
    breaklines=true,
    numbers=left,
    numberstyle=\tiny\color{gray},
    showstringspaces=false,
    tabsize=2,
    frame=single
}

% Title Page Set Up
\title{High Performance Technical Computing Assignment}
\author{Alexis Balayre}
\date{5\textsuperscript{th} February 2024}
\school{\SATM}
\degree{MSc}
\course{Computational Software of Techniques Engineering}
\academicyear{2023 - 2024}

% Supervisors
\supervisor{Dr Irene Moulitsas}

% Copyright
\copyrightyear{2024}

% References
\usepackage[numbers]{natbib} % for nice referencing
\makeatletter % Reference list option change to number and period
\renewcommand\@biblabel[1]{#1.} % from [1] to 1
\makeatother %
\setcitestyle{round} % use round citations

\begin{document}

\frontmatter

% Form Title Pages
\maketitle

% Abstract and Keywords
\begin{abstract}
    Replace with your abstract text of not more than 300 words.
\end{abstract}

% Use single spacing for Table of Contents, List of Figures, etc
{
\clearpage
\singlespacing
% Table of Contents
{
    \tableofcontents
}
\clearpage

% List of Figures
\listoffigures

% List of Tables
\listoftables
}

%% Main Matter
\mainmatter
\pagestyle{fancy}
\fancyhead[L]{\nouppercase{\leftmark}}
\fancyhead[R]{\nouppercase{\rightmark}}

\chapter{Introduction}
High-Performance Computing (HPC) is a branch of computing that uses
supercomputers and server clusters to solve complex, computationally intensive
problems. Unlike a personal computer with a single processor, an HPC system is
made up of many processors working in parallel, considerably increasing
processing capacity. This enables scientists and engineers to carry out
detailed numerical simulations, such as forecasting the weather, modelling
molecules for new medicines or designing aircraft.

Cranfield University has two HPC systems: CRESCENT2 and DELTA. However, this
report will focus exclusively on CRESCENT2. The latter is a HPC cluster
designed to provide computing power to the university community for teaching
and research. The CRESCENT 2 nodes, which are like individual workstations
within the cluster, are equipped with Intel Xeon E5 2620 processors. Each node
contains two processors (or `sockets`), with 16 cores and as many threads,
accompanied by 16 gigabytes of RAM, enabling them to handle large workloads.
Communication between the nodes is ensured by a 128 Gb/s InfiniBand EDR
network, a high-speed connectivity technology that facilitates the rapid
exchange of data, a crucial function for parallel processing of computer tasks.

Sparse matrix-vector multiplication is a fundamental operation in numerical
linear algebra and has numerous applications in science and engineering.

Consider a sparse matrix $M$ of dimensions $m \times n$ and a fat vector $v$ of
dimensions $n \times k$. The objective is to perform the multiplication $M
    \times v$, yielding a result that is of dimensions $m \times k$.

The matrix $M$ is defined as:
\begin{equation}
    M = \begin{pmatrix}
        m_{11} & m_{12} & \cdots & m_{1n} \\
        m_{21} & m_{22} & \cdots & m_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        m_{m1} & m_{m2} & \cdots & m_{mn}
    \end{pmatrix}
\end{equation}\label{eq:sparse-matrix}
where most elements of $M$ are zeros.

The vector $v$ is defined as:
\begin{equation}
    v = \begin{pmatrix}
        v_{11} & v_{12} & \cdots & v_{1k} \\
        v_{21} & v_{22} & \cdots & v_{2k} \\
        \vdots & \vdots & \ddots & \vdots \\
        v_{n1} & v_{n2} & \cdots & v_{nk}
    \end{pmatrix}\label{eq:dense-vector}
\end{equation}

\chapter{Methodology}
\section{Data Structures}
In numerical computation and linear algebra, efficient use of memory and fast
computation are crucial. This is particularly true when working with hollow
matrices and fat vectors.

\subsection{Sparse Matrix}
The sparse matrix is represented in CSR (Compressed Sparse Row) format, which
is particularly effective for storing and manipulating matrices where the
majority of elements are zero. The CSR structure consists of three main
vectors:
\begin{itemize}
    \item \textbf{values}: A vector storing all the non-zero elements of the matrix.
    \item \textbf{rowPtr}: A vector storing the starting index for each element in the
          \textit{values} vector.
    \item \textbf{colIndices}: A vector storing the column indices for each element in the vector \textit{values}.
          \
\end{itemize}

Here is an example of a sparse matrix in CSR format:
\begin{itemize}
    \item \texttt{values} = \{1, 2, 3, 4\}
    \item \texttt{rowPtr} = \{0, 2, 3, 3, 4\}
    \item \texttt{colIndices} = \{0, 2, 1, 3\}
\end{itemize}

This hollow matrix can be visualised as:
\[
    \begin{bmatrix}
        1 & 0 & 2 & 0 \\
        0 & 0 & 3 & 0 \\
        0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 4 \\
    \end{bmatrix}
\]

The \textbf{SparseMatrix} structure is defined in
Appendix~\ref{appendix:data-structures}.

\subsection{fat vector}
Unlike a hollow matrix, a fat vector (illustrated by
equation~\ref{eq:dense-vector}) stores all its elements, including zeros. The
data structure for a fat vector is a two-dimensional array, where each row
represents a separate vector. The \textbf{DenseVector} structure is defined in
Appendix~\ref{appendix:data-structures}.

\section{Sequential Algorithm}
Given a sparse matrix \( M \) in CSR format and a fat vector \( v \), the
product \( M \times v \) is computed as follows:

\begin{algorithm}[H]
    \caption{Sequential algorithm}
    \begin{algorithmic}
        \Require $M$ is an $m \times n$ sparse matrix
        \Require $v$ is an $n \times k$ fat vector
        \Ensure  $Result$ is an $m \times k$ matrix
        \State $Result \gets$ zero matrix of size $m \times k$
        \For{$i \gets 0$ \textbf{to} $m-1$}
        \For{each non-zero element $(j, \text{value})$ in row $i$ of $M$}
        \For{$j \gets 0$ \textbf{to} $k-1$}
        \State $Result[i][l] \gets Result[i][l] + (\text{value} \times v[j][l])$
        \EndFor
        \EndFor
        \EndFor
        \State \Return $Result$
    \end{algorithmic}
\end{algorithm}

\subsection{Complexity Analysis}
Let \( M \) be a sparse matrix of size \( m \times n \) with \( z \) non-zero
elements, stored in CSR format, and \( v \) be a fat vector of size \( n \times
k \). The multiplication \( M \times v \) can be analyzed as follows:

\begin{itemize}
    \item For each row \( i \) of \( M \), the algorithm iterates over its non-zero
          elements.
    \item For each non-zero element \( M_{ij} \), the algorithm performs a multiplication
          with each element of column \( j \) in \( v \), resulting in \( k \)
          multiplications.
    \item These products are accumulated in the corresponding row of the result matrix,
          which has dimensions \( m \times k \).
\end{itemize}

\subsubsection{Temporal Complexity}
The time complexity depends on the number of non-zero elements in the hollow
matrix and the dimension of the fat vector.

\begin{enumerate}
    \item \textbf{Hollow Matrix Row Traversal:} Each row of the matrix is traversed once, so this part has complexity \( O(m) \).
    \item \textbf{Access to non-zero elements:} For each row, the algorithm traverses the non-zero elements, therefore the complexity of traversing all the non-zero elements in the matrix is \( O(z) \).
    \item \textbf{Multiplication et Accumulation:} For each non-zero element, the algorithm performs a multiplication for each column of the fat vector, so this step has complexity \( O(\frac{z}{m} \times k) \). Thus, for all rows, the complexity becomes  \( O(z \times k) \).
\end{enumerate}

Hence, the time complexity of multiplying a sparse matrix in CSR format with a
fat matrix is \( O(z \times k) \).

\subsubsection{Spatial Complexity}

The spatial complexity is related to the amount of memory required by the
algorithm.

\begin{itemize}
    \item \textbf{Sparse Matrix:} The matrix is stored using three main vectors (\( values \), \(colIndices \), \( rowPtr \)). If the matrix has \( z \) non-zero elements, then the spatial complexity of the two vectors \( values \) and \( colIndices \) is \( O(z) \). The vector \( rowPtr \) has a size of \( m + 1 \), so its spatial complexity is \( O(m) \). Thus, the space required for \( M \) in CSR format is \( O(z + z + m) = O(2z + m) \).
    \item \textbf{Fat vector:} The fat vector has a spatial complexity of \( O(n \times k) \).
    \item \textbf{Result Vector:} The result vector has a spatial complexity of \( O(m \times k) \).
\end{itemize}

Considering the storage requirements for the sparse matrix, the dense matrix,
and the result matrix, the overall spatial complexity of the multiplication
operation is \( O(2z + m + n \times k + m \times k) \).

\newpage
\section{Line-Based Parallelism}
This algorithm partitions a sparse matrix into row chunks and distributes these
chunks across multiple processes for parallel computation in a line-based
manner.

\begin{algorithm}[H]
    \caption{Line-based parallel sparse matrix-vector multiplication}
    \begin{algorithmic}
        \Require $M$ is an $m \times n$ sparse matrix
        \Require $v$ is an $n \times k$ dense vector
        \Require $numProcs$ is the number of processes
        \Require $rank$ is the rank of the current process
        \Ensure $Result$ is a part of the $m \times k$ matrix computed by this process

        \State $rowsPerProc \gets m / numProcs$
        \State $extraRows \gets m\mod numProcs$
        \State $startRow \gets rank \times rowsPerProc + \min(rank, extraRows)$
        \State $endRow \gets startRow + rowsPerProc + (rank < extraRows)$
        \State $localRows \gets endRow - startRow$
        \State $Result \gets$ zero matrix of size $localRows \times k$

        \For{$i \gets startRow$ \textbf{to} $endRow-1$}
        \For{each non-zero element $(j, \text{value})$ in row $i$ of $M$}
        \For{$l \gets 0$ \textbf{to} $k-1$}
        \State $Result[i - startRow][l] \gets Result[i - startRow][l] + (\text{value} \times v[j][l])$
        \EndFor
        \EndFor
        \EndFor

        \If{$rank = 0$}
        \State $FinalResult \gets$ zero matrix of size $m \times k$
        \EndIf

        \State Define $recvCounts[numProcs]$, $displacements[numProcs]$
        \State Compute $recvCounts$ and $displacements$ based on $localRows$ for all processes

        \If{$rank != 0$}
        \State Send $Result$ to process $0$
        \Else
        \For {$p \gets 1$ \textbf{to} $numProcs-1$}
        \State Integrates received partial $Results$ into $FinalResult$ based on $displacements$
        \EndFor
        \EndIf

        \State \textbf{if} $rank = 0$ \textbf{then} \Return $FinalResult$
    \end{algorithmic}
\end{algorithm}

\subsection{Complexity Analysis}
\subsubsection{Temporal Complexity}
The temporal (or time) complexity analysis of the sparse matrix-fat vector
multiplication using MPI in a distributed-memory parallel environment involves
understanding the computational steps required for each part of the process.
The main components are as follows:

\begin{enumerate}
    \item \textbf{Initialisation and Setup}:
          MPI initialisation and calculation of rows per process have a negligible time
          complexity compared to the actual computation, so they can be considered as \(
          O(1) \).

    \item \textbf{Local Computation}:
          \begin{itemize}
              \item Each process computes the multiplication for its assigned subset of rows.
              \item If the sparse matrix has \( z \) non-zero elements in total, and these elements
                    are evenly distributed across \( m \) rows, each process handles approximately
                    \( \frac{z}{\text{worldSize}} \) non-zero elements.
              \item The computation for each non-zero element involves accessing the element and
                    its corresponding column in the dense vector, followed by a multiplication and
                    an addition. This operation is \( O(1) \).
              \item Thus, the local computation for each process has a time complexity of \(
                    O\left(\frac{z}{\text{worldSize}}\right) \).
          \end{itemize}

    \item \textbf{Communication (\texttt{MPI\_Gatherv})}:
          \begin{itemize}
              \item The complexity of the \texttt{MPI\_Gatherv} operation depends on the
                    implementation of the MPI library and the underlying network architecture.
              \item In general, gathering operations can be assumed to have a logarithmic
                    complexity with respect to the number of processes, i.e., \(
                    O(\log(\text{worldSize})) \), but this can vary.
              \item The amount of data transferred per process is proportional to the size of the
                    local result, which is \( O\left(\frac{m}{\text{worldSize}} \times k\right) \).
          \end{itemize}

    \item \textbf{Final Assembly}:
          \begin{itemize}
              \item The root process assembles the final result matrix. This step is essentially a
                    concatenation of the results from each process and has a complexity linear to
                    the total size of the result matrix, which is \( O(m \times k) \).
          \end{itemize}
\end{enumerate}

Considering the parallel nature of the computation, the dominant factor in the
time complexity is the local computation performed by each process, which is \(
O\left(\frac{z}{\text{worldSize}}\right) \). The communication step's
complexity depends on the MPI implementation and network, but the data volume
transferred per process affects this step. The final assembly in the root
process is also significant but does not exceed \( O(m \times k) \). Therefore,
the overall time complexity of the algorithm can be approximated as \(
O\left(\frac{z}{\text{worldSize}} + \log(\text{worldSize}) + m \times k\right)
\), with the understanding that the actual performance can be influenced by
factors like network latency, bandwidth, and the distribution of non-zero
elements in the sparse matrix.

\subsubsection{Spatial Complexity}
The spatial complexity analysis of the algorithm performing sparse matrix-fat
vector multiplication in a distributed-memory parallel environment using MPI
can be understood by considering the memory requirements for storing the sparse
matrix, the dense vector, and the final result, as well as the additional
storage required for parallel processing. The breakdown is as follows:

\begin{enumerate}
    \item \textbf{Storage of Sparse Matrix (\texttt{sparseMatrix}):}
          \begin{itemize}
              \item The sparse matrix is stored in CSR format, which includes three arrays:
                    \begin{itemize}
                        \item \texttt{values}: Contains all non-zero elements of the matrix. If the matrix has \( z \) non-zero elements, this array takes \( O(z) \) space.
                        \item \texttt{colIndices}: Stores column indices for each non-zero element, also taking \( O(z) \) space.
                        \item \texttt{rowPtr}: Contains \( m + 1 \) elements for an \( m \times n \) matrix, requiring \( O(m + 1) \) space.
                    \end{itemize}
              \item Total space for the sparse matrix is therefore \( O(2z + m) \).
          \end{itemize}

    \item \textbf{Storage of Dense Vector (\texttt{denseVector}):}
          \begin{itemize}
              \item The dense vector (or fat vector) is a 2D array of size \( n \times k \). It
                    requires \( O(n \times k) \) space.
          \end{itemize}

    \item \textbf{Local Result Vector (\texttt{localResult}):}
          \begin{itemize}
              \item Each process computes a local result vector of size proportional to the number
                    of rows it handles times the number of columns in the dense vector. In the
                    worst case, this is \( O\left(\frac{m}{\text{worldSize}} + 1 \times k\right) \)
                    per process.
          \end{itemize}

    \item \textbf{Gathered Results (\texttt{gatheredResults}):}
          \begin{itemize}
              \item In the root process, the gathered results from all processes are combined. This
                    requires space equivalent to the size of the final result matrix, which is \(
                    O(m \times k) \).
          \end{itemize}

    \item \textbf{Final Result Matrix (\texttt{finalResult}):}
          \begin{itemize}
              \item The final result matrix, which is only constructed in the root process, is of
                    size \( m \times k \), requiring \( O(m \times k) \) space.
          \end{itemize}

    \item \textbf{Auxiliary Data for MPI Operations:}
          \begin{itemize}
              \item Arrays like \texttt{recvCounts} and \texttt{displacements} are used for MPI
                    communication. Their sizes are proportional to the number of processes, which
                    is typically much smaller than the size of the matrix or vectors. Therefore,
                    they add a relatively negligible \( O(\text{worldSize}) \) space complexity.
          \end{itemize}
\end{enumerate}

The spatial complexity of the algorithm is dominated by the storage
requirements for the sparse matrix, the dense vector, and the final result
matrix. Thus, the overall spatial complexity is approximately \( O(2z + m + n
\times k + m \times k) \). It is important to note that in a distributed-memory
environment, the memory usage is distributed across multiple processes, which
can reduce the memory burden on individual nodes.

\newpage
\section{Column-Wise Parallelism}
This algorithm distributes the non-zero elements of a sparse matrix among
different processes, enabling parallel computation focused on each non-zero
element.

\begin{algorithm}
    \caption{Column-wise Parallelization using MPI for Sparse Matrix-Fat Vector Multiplication}
    \begin{algorithmic}
        \Require $M$ is a sparse matrix of dimensions $m \times n$
        \Require $v$ is a dense vector of dimensions $n \times k$
        \Require $numProcs$ is the number of processes
        \Require $rank$ is the rank of the current process
        \Ensure $PartialResult$ is part of the resulting $m \times k$ matrix computed by this process

        \State $colsPerProc \gets k / numProcs$
        \State $extraCols \gets k \% numProcs$
        \State $startCol \gets rank \times colsPerProc + \min(rank, extraCols)$
        \State $endCol \gets (rank < numProcs - 1) ? startCol + colsPerProc : startCol + colsPerProc + extraCols$
        \State $PartialResult \gets$ zero matrix of size $m \times (endCol - startCol)$

        \For{$col \gets startCol$ \textbf{to} $endCol - 1$}
        \For{$i \gets 0$ \textbf{to} $m - 1$}
        \State $sum \gets 0$
        \For{each non-zero element $(j, \text{value})$ in row $i$ of $M$}
        \State $sum \gets sum + (\text{value} \times v[j][col])$
        \EndFor
        \State $PartialResult[i][col - startCol] \gets sum$
        \EndFor
        \EndFor

        \If{$rank = 0$}
        \State $FinalResult \gets$ zero matrix of size $m \times k$
        \EndIf

        \State $MPI\_Gatherv(PartialResult, localSize, MPI\_DOUBLE, FinalResult, recvCounts, displacements, MPI\_DOUBLE, 0, MPI\_COMM\_WORLD)$

        \If{$rank = 0$}
        \State Integrate $PartialResult$ into $FinalResult$
        \Return $FinalResult$
        \EndIf
    \end{algorithmic}
\end{algorithm}

\subsection{Complexity Analysis}
\subsubsection{Temporal Complexity}

To analyse the temporal (or time) complexity of the sparse matrix-fat vector
multiplication using a column-wise parallel approach with MPI, we need to
consider the computation and communication steps involved in the process. The
breakdown is as follows:

\begin{enumerate}
    \item \textbf{Local Computation:}
          \begin{itemize}
              \item Each MPI process computes a portion of the final matrix, responsible for a
                    subset of columns. The number of columns processed by each process is roughly
                    \( \text{colsPerProcess} = \frac{\text{vecCols}}{\text{worldSize}} \), with
                    some processes handling extra columns if \( \text{vecCols} \) is not perfectly
                    divisible by \( \text{worldSize} \).
              \item For each column, the process computes the product with every row of the sparse
                    matrix. If the sparse matrix has \( z \) non-zero elements in total, then, on
                    average, each process handles approximately \( \frac{z}{\text{worldSize}} \)
                    non-zero elements.
              \item The computation involves accessing the element, performing a multiplication,
                    and accumulating the result. These operations for each non-zero element are \(
                    O(1) \).
              \item Therefore, the local computation for each process has a time complexity of \(
                    O\left(\frac{z}{\text{worldSize}}\right) \).
          \end{itemize}

    \item \textbf{Communication (\texttt{MPI\_Gatherv}):}
          \begin{itemize}
              \item The \texttt{MPI\_Gatherv} operation is used to gather the local results from
                    each process into the root process. The complexity of this operation depends on
                    the implementation of MPI and the underlying network architecture.
              \item Generally, the gather operation can be assumed to have a complexity that grows
                    with the number of processes and the amount of data being communicated. If the
                    gathered data from each process is large, the communication time can become
                    significant.
              \item The amount of data transferred per process is proportional to the number of
                    rows in the matrix and the number of columns processed, which is \( O(m \times
                    \text{colsPerProcess}) \).
          \end{itemize}

    \item \textbf{Final Assembly:}
          \begin{itemize}
              \item The root process assembles the final result matrix. This step is essentially a
                    concatenation of results from each process and is linearly proportional to the
                    size of the final matrix, \( O(m \times k) \).
          \end{itemize}
\end{enumerate}

The dominant factor in the time complexity is the local computation performed
by each process, which is \( O\left(\frac{z}{\text{worldSize}}\right) \). The
communication step can also be significant, especially if the network bandwidth
is limited or if the size of the data being communicated is large. The final
assembly in the root process is also linearly dependent on the size of the
final result matrix. Therefore, the overall time complexity of the algorithm
can be approximated as \( O\left(\frac{z}{\text{worldSize}} +
\text{communication cost} + m \times k\right) \), with the understanding that
actual performance can vary based on factors like network performance,
distribution of non-zero elements in the sparse matrix, and the specific
implementation of MPI.

\subsubsection{Spatial Complexity}
The spatial complexity of the given algorithm, which performs sparse matrix-fat
vector multiplication in a distributed-memory environment using a column-wise
parallel approach, can be analysed by considering the memory requirements for
storing the matrix, the vector, and the intermediate and final computational
results. The breakdown is as follows:
\begin{enumerate}
    \item \textbf{Sparse Matrix Storage:}
          \begin{itemize}
              \item The sparse matrix is stored in a CSR format, which includes arrays for non-zero
                    values, column indices, and row pointers. If the matrix has \( z \) non-zero
                    elements, then the space required is approximately \( O(z) \) for the values,
                    \( O(z) \) for the column indices, and \( O(m + 1) \) for the row pointers,
                    where \( m \) is the number of rows. Therefore, the total space requirement for
                    the sparse matrix is \( O(2z + m) \).
          \end{itemize}

    \item \textbf{Dense Vector Storage:}
          \begin{itemize}
              \item The dense vector (or fat vector) is a 2D array of size \( n \times k \),
                    requiring \( O(n \times k) \) space.
          \end{itemize}

    \item \textbf{Local Result Vector:}
          \begin{itemize}
              \item Each process computes a local result vector for its assigned columns. The size
                    of this local result vector is proportional to the number of rows in the sparse
                    matrix and the number of columns processed by each process. The maximum size is
                    \( O(m \times \text{colsPerProcess}) \), where \(\text{colsPerProcess}\) is the
                    number of columns processed by each process.
          \end{itemize}

    \item \textbf{Gathered Results:}
          \begin{itemize}
              \item In the root process, the final gathered result needs to be stored. This is
                    essentially the entire output matrix, which has a size of \( m \times k \),
                    hence requiring \( O(m \times k) \) space.
          \end{itemize}

    \item \textbf{Auxiliary Arrays for MPI Operations:}
          \begin{itemize}
              \item Arrays such as \texttt{recvCounts} and \texttt{displacements} are used in the
                    MPI Gather operation. The size of these arrays is proportional to the number of
                    processes (i.e., \(\text{worldSize}\)). However, this is generally small
                    compared to the size of the matrices and vectors, so their space contribution
                    is relatively minor.
          \end{itemize}
\end{enumerate}

Considering the storage requirements for the sparse matrix, the dense vector,
the local results, and the final gathered results, the overall spatial
complexity of the algorithm is approximately \( O(2z + m + n \times k + m
\times \text{colsPerProcess} + m \times k) \). It is important to note that in
a distributed-memory setting, this memory usage is spread across multiple
processes, reducing the memory load on any single node.

\newpage
\section{Non-Zero Element Parallelism}
This algorithm combines line-based and non-zero element-based approaches by
distributing chunks of rows to each process and then performing parallel
computations on the non-zero elements within those chunks.

\begin{algorithm}
    \caption{Non-Zero Element Parallelization using MPI for Sparse Matrix-Fat Vector Multiplication}
    \begin{algorithmic}
        \Require $M$ is an $m \times n$ sparse matrix stored in a format that allows iterating over non-zero elements (e.g., COO, CSR)
        \Require $v$ is an $n \times k$ vector
        \Require $numProcs$ is the number of processes
        \Require $rank$ is the rank of the current process
        \Ensure  $PartialResult$ is a part of the $m \times k$ matrix computed by this process

        \State $numNonZeroElements \gets$ total number of non-zero elements in $M$
        \State $elementsPerProc \gets numNonZeroElements / numProcs$
        \State $startIndex \gets rank \times elementsPerProc$
        \State $endIndex \gets startIndex + elementsPerProc$
        \State $PartialResult \gets$ zero matrix of size $m \times k$

        \State $NonZeroElements \gets$ list of non-zero elements in $M$ from $startIndex$ to $endIndex - 1$
        \For{each $(i, j, \text{value})$ in $NonZeroElements$}
        \For{$l \gets 0$ \textbf{to} $k-1$}
        \State $PartialResult[i][l] \gets PartialResult[i][l] + (\text{value} \times v[j][l])$
        \EndFor
        \EndFor

        \If{$rank \neq 0$}
        \State Send $PartialResult$ to process $0$
        \Else
        \State $FinalResult \gets$ zero matrix of size $m \times k$
        \State Copy $PartialResult$ into $FinalResult$
        \For{$p \gets 1$ \textbf{to} $numProcs-1$}
        \State Receive partial $PartialResult$ from process $p$
        \State Add received $PartialResult$ into $FinalResult$
        \EndFor
        \EndIf

        \State \textbf{if} $rank = 0$ \textbf{then} \Return $FinalResult$
    \end{algorithmic}
\end{algorithm}

\subsection{Complexity Analysis}
\subsubsection{Temporal Complexity}

\begin{itemize}
    \item \textbf{MPI Initialisation and Rank and Size Determination:} As with other MPI-based algorithms, this step has a complexity of approximately \( O(1) \).

    \item \textbf{Scattering Chunks of Rows of \( M \) to Each Process:} This step distributes parts of the matrix to different processes. Its complexity depends on the number of rows and the distribution method, typically around \( O(\frac{m}{p}) \), where \( m \) is the number of rows and \( p \) is the number of processes.

    \item \textbf{Scatter of Vector \( v \) to All Processes:} This operation generally has a complexity of \( O(n) \), where \( n \) is the size of the vector.

    \item \textbf{Local Computations for Non-Zero Elements:} Each process computes the products for the non-zero elements in its assigned rows. Assuming an even distribution of non-zero elements, the complexity for each process is approximately \( O(\frac{n_{nz}}{p}) \).

    \item \textbf{Gather of Local Results \( r_{local} \) into Final Result Vector \( r \):} This step combines the partial results from all processes and typically has a complexity proportional to the total number of elements in \( r \).
\end{itemize}

\subsubsection{Spatial Complexity}

\begin{itemize}
    \item \textbf{Storage of Sparse Matrix and fat vector:} The overall storage requirements remain \( O(n_{nz} + m + n) \), as in other sparse matrix-vector multiplication methods.

    \item \textbf{Local Result Vectors \( r_{local} \):} Each process stores a local result vector for its chunk of rows, with the size depending on the distribution of rows and non-zero elements.
\end{itemize}

\subsection*{Complexity and Considerations}

Each of these parallel algorithms aims to exploit different aspects of
parallelism, with the primary goal of reducing the overall computation time.
The actual performance gain depends on the characteristics of the sparse
matrix, the number of available processing units, and the specific
implementation details. Moreover, care must be taken to manage concurrency
issues, such as race conditions and proper synchronization, to ensure correct
and efficient execution.

\newpage

\chapter{Results and Discussion}

\section{HPC Environmental Impact}
The LUMI supercomputer, based at the CSC-IT Center for Science in Finland,
represents a milestone in the field of high-performance computing (HPC), not
only because of its computing power, but also because of its approach to
environmental sustainability. LUMI, one of EuroHPC's world-class
supercomputers, began operating in 2021 and is expected to reach full capacity
in 2023. It features an environmentally-friendly design and is considered to be
one of the most energy-efficient data centres in the world.

Irina Kupiainen, who works as Programme Director for the Open Scholarship
Innovation Program at the CSC, plays an important role in the development of
policies relating to high-performance computing and open science. With a strong
background in international policy and experience in various government and
research organisations, Irina Kupiainen leads EU public affairs at the CSC,
focusing on policy and international collaboration, particularly in the area of
open science.

The impact of supercomputing on the environment is a major concern, not least
because of the high energy demands of these systems. However, LUMI is an
example of how HPC can make a positive contribution to environmental
sustainability. It runs on 100\% renewable energy and makes efficient use of
waste heat, which can heat up to 20\% of the homes in the surrounding city.
This approach not only reduces the carbon footprint, but also demonstrates
HPC's potential to meet climate neutrality targets.

Furthermore, sustainability measures in HPC are not limited to energy
consumption and waste heat management. The entire life cycle of the machine
needs to be taken into account, including construction, modularity,
scalability, recycling and reuse of materials. This holistic view can
contribute to the development of a circular economy, supporting sustainability
to its full potential.

In conclusion, the LUMI supercomputer, led by experts such as Irina Kupiainen
and others at the CSC-IT Center for Science, illustrates how supercomputing can
be both a powerful tool for scientific progress and a leader in environmental
sustainability. By harnessing renewable energy sources, making efficient use of
waste heat and taking into account the full lifecycle of HPC systems, LUMI is
setting a precedent for how high-performance computing can contribute to a
greener, more sustainable future.

\chapter{Conclusion}

\bibliographystyle{CranfieldNumbered}
\bibliography{CUCitations}

\appendix
\chapter{Documentation}

\begin{subappendices}
    \section{Project tree}
    \begin{lstlisting}[breaklines=true, basicstyle=\small]
    lib/
        collecting.py
        processing.py
        storing.py
    scripts/
        get_iam_credentials.sh
        start_spark_job.sh
    services/
        get_iam_credentials.service
        spark_python_job.service
    test/
        artillery_load_test.yml
        monitoring.py
        metrics.csv
        results.json
        visualisation_load_test.ipynb
    main.py
    README.md
    requirements.txt
    \end{lstlisting}

    \section{Getting Started}
    To run the program, follow these steps:
    \begin{enumerate}
        \itemindent=17.87pt
        \item Create a virtual environment using \texttt{python3 -m venv venv}.
        \item Activate the virtual environment using \texttt{source venv/bin/activate}.
        \item Install the required dependencies using \texttt{pip3 install -r
                  requirements.txt}.
        \item Run the program using \texttt{python3 main.py}.
        \item Visualise the results using \texttt{visualisation.ipynb} (Jupyter Notebook).
    \end{enumerate}

    \section{Detailed Features of Functions}
    \begin{description}
        \item \texttt{collecting.py}
              \begin{itemize}
                  \item \texttt{fetch\_sensors\_data(sparkSession)}: Function to ingest the latest data from the sensors and returns it as a Spark DataFrame.
              \end{itemize}

        \item \texttt{processing.py}
              \begin{itemize}
                  \item \texttt{get\_aqi\_value\_p25(value)}: Function for calculating the AQI value for PM2.5.
                  \item \texttt{get\_aqi\_value\_p10(value)}: Function for calculating the AQI value for PM10.
                  \item \texttt{computeAQI(df)}: Function for calculating the AQI value for each particulate matter sensor and returning the DataFrame with the AQI column.
              \end{itemize}

        \item \texttt{storing.py}
              \begin{itemize}
                  \item \texttt{keepOnlyUpdatedRows(database\_name, table\_name, df)}: Function for keeping only the rows that have been updated in the DataFrame.
                  \item \texttt{\_print\_rejected\_records\_exceptions(err)}: Internal function for printing the rejected records exceptions.
                  \item \texttt{write\_records(database\_name, table\_name, client, records)}: Internal function for writing a batch of records to the Timestream database.
                  \item \texttt{writeToTimestream(database\_name, table\_name, partionned\_df)}: Function for writing the DataFrame to the Timestream database.
              \end{itemize}
    \end{description}
\end{subappendices}

\chapter{Source Codes}
\begin{subappendices}
    \section{Data Structures}\label{appendix:data-structures}
    Data stuctures of the sparse matrix and fat vector.
    \lstinputlisting[style=cstyle]{../Source Code/MatrixDefinitions.h}

    \newpage

    \section{Sequential Algorithm}\label{appendix:sequential}
    Sequential algorithm for multiplying a sparse matrix by a fat vector.
    \subsection{Declaration File}
    \lstinputlisting[style=cstyle]{../Source Code/SparseMatrixDenseVectorMultiply.h}
    \subsection{Implementation File}
    \lstinputlisting[style=cstyle]{../Source Code/SparseMatrixDenseVectorMultiply.cpp}

    \section{Line-Based Parallelism}\label{appendix:line-based}
    Parallel algorithm for multiplying a sparse matrix by a fat vector using
    line-based parallelism.
    \subsection{Declaration File}
    \lstinputlisting[style=cstyle]{../Source Code/SparseMatrixDenseVectorMultiplyRowWise.h}
    \subsection{Implementation File}
    \lstinputlisting[style=cstyle]{../Source Code/SparseMatrixDenseVectorMultiplyRowWise.cpp}

    \section{Column-Wise Parallelism}\label{appendix:column-based}
    Parallel algorithm for multiplying a sparse matrix by a fat vector using
    column-wise parallelism.
    \subsection{Declaration File}
    \lstinputlisting[style=cstyle]{../Source Code/SparseMatrixDenseVectorMultiplyColumnWise.h}
    \subsection{Implementation File}
    \lstinputlisting[style=cstyle]{../Source Code/SparseMatrixDenseVectorMultiplyColumnWise.cpp}

    \section{Non-Zero Element Parallelism}\label{appendix:non-zero}
    Parallel algorithm for multiplying a sparse matrix by a fat vector using
    non-zero element parallelism.
    \subsection{Declaration File}
    \lstinputlisting[style=cstyle]{../Source Code/SparseMatrixDenseVectorMultiplyNonZeroElement.h}
    \subsection{Implementation File}
    \lstinputlisting[style=cstyle]{../Source Code/SparseMatrixDenseVectorMultiplyNonZeroElement.cpp}

    \section{Utility Functions}\label{appendix:utility}
    Utility functions used by the main file.
    \subsection{Declaration File}
    \lstinputlisting[style=cstyle]{../Source Code/utils.h}
    \subsection{Implementation File}
    \lstinputlisting[style=cstyle]{../Source Code/utils.cpp}

    \section{Main File}\label{appendix:main}
    Main file for running the different algorithms and comparing their performance.
    \lstinputlisting[style=cstyle]{../Source Code/main.cpp}

    \section{Scripts}
    \subsection{MPI Submission Script} \label{appendix:mpi-sub}
    Bash script to submit an MPI job to the cluster.
    \lstinputlisting[style=bashstyle]{../Source Code/scripts/mpi.sub}

    \newpage
    \subsection{Batch Test Script}\label{appendix:batch-test}
    Bash script to submit multiple MPI jobs to the cluster.
    \lstinputlisting[style=bashstyle]{../Source Code/scripts/batch_test.sh}

    \newpage
    \subsection{Get CSV Script}\label{appendix:get-csv}
    Bash script to analyse all job results files and extract the relevant
    information to create a CSV file. \lstinputlisting[style=bashstyle]{../Source
        Code/scripts/get_csv_all.sh}

\end{subappendices}

\end{document}

